{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GMM Test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNofhEvFHxUuHSFsP8bW/Mh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nwanna-Joseph/GaussianMixtureModel/blob/main/GMM_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class KMeans():\n",
        "    def __init__(self, K, epsilon):\n",
        "        self.K = K\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def initialize(self, X, K):\n",
        "        \"\"\"\n",
        "        Initilize K random Centroids\n",
        "\n",
        "        \"\"\"\n",
        "        indices = np.arange(len(X))\n",
        "        np.random.shuffle(indices)\n",
        "        X = np.array(X)\n",
        "        X = X[indices]\n",
        "        Centroids = X[:K]\n",
        "        return Centroids\n",
        "\n",
        "    def EuclideanDistance(self, x1, x2):\n",
        "        \"\"\"\n",
        "        The sum of the squared differences of the elements\n",
        "\n",
        "        \"\"\"\n",
        "        dist = np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "        return dist\n",
        "\n",
        "    def Classify(self, centroids, X):\n",
        "        \"\"\"\n",
        "        Classify item to the cluster with minimum distance\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        distances = np.zeros((len(centroids), len(X)))\n",
        "        for i in range(len(centroids)):\n",
        "            for j in range(len(X)):\n",
        "                distances[i][j] = self.EuclideanDistance(centroids[i], X[j])\n",
        "\n",
        "        labels = np.argmin(distances, axis=0)\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def update_centroids(self, Assignments, X):\n",
        "        \"\"\"\n",
        "        Calculate the new means\n",
        "\n",
        "        \"\"\"\n",
        "        centroids = np.zeros((self.K, X.shape[1]))\n",
        "        \"\"\"\n",
        "        for i in range(self.K):\n",
        "          arr = np.array([X[j] for j,q in enumerate(Assignments) if q==i])\n",
        "          centroids[i] = np.sum(arr, axis=0)/len(arr)\n",
        "        \"\"\"\n",
        "        for i in range(self.K):\n",
        "            centroids[i] = np.mean(X[Assignments == i], axis=0)\n",
        "\n",
        "        return centroids\n",
        "\n",
        "    def fit(self, data):\n",
        "        \"\"\"\n",
        "        Fit the K-Means algorithm until no changes happens or changes < epsilon\n",
        "\n",
        "        \"\"\"\n",
        "        old_centroids = self.initialize(data, self.K)\n",
        "        assignments = self.Classify(old_centroids, np.array(data))\n",
        "\n",
        "        error = 1\n",
        "\n",
        "        while (error >= self.epsilon):\n",
        "            new_centroids = self.update_centroids(assignments, np.array(data))\n",
        "            assignments = self.Classify(new_centroids, np.array(data))\n",
        "            error = np.mean(np.abs(new_centroids - old_centroids))\n",
        "            print(\"loss\", error)\n",
        "            old_centroids = new_centroids\n",
        "\n",
        "        return assignments, new_centroids\n",
        "\n",
        "    def visualize_clusters(self, X, assignments, new_centroids):\n",
        "\n",
        "        num_centers = max(assignments)\n",
        "\n",
        "        for i in range(num_centers + 1):\n",
        "            c_ = [index for index, value in enumerate(assignments) if value == i]\n",
        "            c = X.loc[c, :]\n",
        "\n",
        "            plt.scatter(\n",
        "                c.iloc[:, 0], c.iloc[:, 1],\n",
        "                s=50,\n",
        "                marker='s', edgecolor='black',\n",
        "                label='cluster' + str(i + 1)\n",
        "            )\n",
        "\n",
        "        # plot the centroids\n",
        "\n",
        "        plt.scatter(\n",
        "            new_centroids[:, 0], new_centroids[:, 1],\n",
        "            s=250, marker='*',\n",
        "            c='red', edgecolor='black',\n",
        "            label='centroids'\n",
        "        )\n",
        "\n",
        "        plt.legend(scatterpoints=1)\n",
        "        plt.grid()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "uqXeP2z6feY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAxLwIBFshW-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class Gaussian:\n",
        "  def __init__(self, mean, std, weight):\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "    self.weight = weight\n",
        "\n",
        "    self.weights = []\n",
        "\n",
        "class GMM:\n",
        "  # https://youtu.be/Dn6b9fCIUpM, https://youtu.be/q71Niz856KE\n",
        "\n",
        "  def __init__(self,unlabelled_data, centers):\n",
        "    self.k = len(centers)\n",
        "    self.data = unlabelled_data\n",
        "    self.gaussians = [Gaussian(i,1,1/self.k) for i in centers]\n",
        "\n",
        "  def train(self,iter = 100):\n",
        "    while(iter):\n",
        "      self.assignGaussianChanceToDatapoints()\n",
        "      # print(\"self.gaussians[0].weights\",len(self.gaussians[0].weights),np.array(self.gaussians[0].weights))\n",
        "      self.updateGaussianParameters()\n",
        "      iter -= 1\n",
        "      \n",
        "\n",
        "  def calculateLikeliHood(self, data, gaussian):\n",
        "    #PDF of gaussian distribution\n",
        "    key = ( (data - gaussian.mean ) / np.abs(gaussian.std) )\n",
        "    exp_value = ( (key * key) / -2 ) \n",
        "    return ( np.exp( exp_value ) ) / ( np.sqrt(2 * np.pi) * (gaussian.std) )\n",
        "\n",
        "\n",
        "  def assignGaussianChanceToDatapoints(self):\n",
        "    # an (N x M) matrix where N = number of datapoints and M = number of Gaussians\n",
        "\n",
        "    for _gaussian in self.gaussians:\n",
        "      _gaussian.weights = [] \n",
        "\n",
        "    for _point in self.data:\n",
        "      #for each point\n",
        "      point_weights = []\n",
        "      point_weights_sum = 0\n",
        "      for _gaussian in self.gaussians:\n",
        "        #calculate the sum across all gaussians\n",
        "        gaussian_weight = self.calculateLikeliHood(_point,_gaussian) * _gaussian.weight\n",
        "        point_weights_sum += np.sum(gaussian_weight)\n",
        "        point_weights.append(gaussian_weight)\n",
        "\n",
        "      for i, _gaussian in enumerate(self.gaussians):\n",
        "        # print(\"lll\")\n",
        "        _gaussian.weights.append(point_weights[i]/point_weights_sum)\n",
        "\n",
        "\n",
        "  def updateGaussianParameters(self):\n",
        "    # print(np.array(self.gaussians[0].weights))\n",
        "    for _gaussian in self.gaussians:\n",
        "      sum_of_weights = np.sum(_gaussian.weights)\n",
        "      _gaussian.mean = np.sum( (_gaussian.weights * self.data) / sum_of_weights, axis=0 )\n",
        "      _gaussian.std = np.sqrt( np.sum( np.array(_gaussian.weights) * np.linalg.norm(self.data - _gaussian.mean ) , axis = 0) / sum_of_weights )\n",
        "      _gaussian.weight = sum_of_weights / len(self.data)\n",
        "    # pass\n",
        "\n",
        "  def calculateMaxLikelihoodOfMean(self, data):\n",
        "    #https://youtu.be/Dn6b9fCIUpM\n",
        "    return np.sum(data)/len(data)\n",
        "  \n",
        "  def calculateMaxLikelihoodOfSTD(self, data, mean_constant):\n",
        "    #https://youtu.be/Dn6b9fCIUpM\n",
        "    return np.sqrt( (np.sum( np.linalg(data - mean_constant))) / len(data) )\n",
        "\n",
        "  def predict(self, unlabelled_data):\n",
        "    #calculate the chance of each datapoint belonging to each k Gaussian\n",
        "    pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "train_data = data.data\n",
        "train_data"
      ],
      "metadata": {
        "id": "Gz102bMdtkOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T7bwplb5wKg",
        "outputId": "d3187627-f91d-4704-97e7-801cbca6c275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qX = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])\n",
        "\n",
        "# kmeans = KMeans( K=2, epsilon = 0.0001)\n",
        "# labels, centers = kmeans.fit(train_data)\n",
        "\n",
        "centers = np.array([\n",
        "           [5.00566038, 3.36981132, 1.56037736, 0.29056604],\n",
        "           [6.30103093, 2.88659794, 4.95876289, 1.69587629]\n",
        "          ])\n",
        "print(f\"centers:\\n {centers}\")\n",
        "gmm = GMM(train_data, centers)\n",
        "gmm.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EGEjv5TOT-R",
        "outputId": "9b029dae-ad76-421e-a5c5-0350f8279cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "centers:\n",
            " [[5.00566038 3.36981132 1.56037736 0.29056604]\n",
            " [6.30103093 2.88659794 4.95876289 1.69587629]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gmm.gaussians[0].std"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS7ooRc5AAUI",
        "outputId": "6a2e0bf3-411f-43c2-bfee-b51979ee765c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3.60689391, 4.75644273, 4.49910027, 4.88608838])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for g in gmm.gaussians:\n",
        "  print(g.std)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBXDtQbdaDUt",
        "outputId": "9864cc8d-1443-4327-9617-10d502993c42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3.60689391 4.75644273 4.49910027 4.88608838]\n",
            "[3.60689391 4.75644273 4.49910027 4.88608838]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])\n",
        "gm = GaussianMixture(n_components=2, random_state=0).fit(train_data)\n",
        "gm\n",
        "# array([[10.,  2.],\n",
        "#        [ 1.,  2.]])\n",
        "# gm.predict([[0, 0], [12, 3]])\n",
        "# array([1, 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0IgS-6MOwkX",
        "outputId": "5f0a9dbe-fe0f-42f1-ca5e-c98953a25949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianMixture(n_components=2, random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gm = GaussianMixture(n_components=2, random_state=0).fit(train_data)\n",
        "gm.means_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvqKsMjMTd0q",
        "outputId": "baf2927c-b0e0-4e3c-f90f-96d115ece4fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.00600639, 3.4280142 , 1.46200203, 0.24599932],\n",
              "       [6.26198886, 2.87199642, 4.90597719, 1.67599129]])"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "class GMMl:\n",
        "    '''\n",
        "        This class is the implementation of the Gaussian Mixture Models \n",
        "        inspired by sci-kit learn implementation.\n",
        "    '''\n",
        "    def __init__(self, n_components, max_iter = 100, comp_names=None):\n",
        "        '''\n",
        "            This functions initializes the model by seting the following paramenters:\n",
        "                :param n_components: int\n",
        "                    The number of clusters in which the algorithm must split\n",
        "                    the data set\n",
        "                :param max_iter: int, default = 100\n",
        "                    The number of iteration that the algorithm will go throw to find the clusters\n",
        "                :param comp_names: list of strings, default=None\n",
        "                    In case it is setted as a list of string it will use to\n",
        "                    name the clusters\n",
        "        '''\n",
        "        self.n_componets = n_components\n",
        "        self.max_iter = max_iter\n",
        "        if comp_names == None:\n",
        "            self.comp_names = [f\"comp{index}\" for index in range(self.n_componets)]\n",
        "        else:\n",
        "            self.comp_names = comp_names\n",
        "        # pi list contains the fraction of the dataset for every cluster\n",
        "        self.pi = [1/self.n_componets for comp in range(self.n_componets)]\n",
        "\n",
        "    def multivariate_normal(self, X, mean_vector, covariance_matrix):\n",
        "        '''\n",
        "            This function implements the multivariat normal derivation formula,\n",
        "            the normal distribution for vectors it requires the following parameters\n",
        "                :param X: 1-d numpy array\n",
        "                    The row-vector for which we want to calculate the distribution\n",
        "                :param mean_vector: 1-d numpy array\n",
        "                    The row-vector that contains the means for each column\n",
        "                :param covariance_matrix: 2-d numpy array (matrix)\n",
        "                    The 2-d matrix that contain the covariances for the features\n",
        "        '''\n",
        "        return (2*np.pi)**(-len(X)/2)*np.linalg.det(covariance_matrix)**(-1/2)*np.exp(-np.dot(np.dot((X-mean_vector).T, np.linalg.inv(covariance_matrix)), (X-mean_vector))/2)\n",
        "\n",
        "    def fit(self, X):\n",
        "        '''\n",
        "            The function for training the model\n",
        "                :param X: 2-d numpy array\n",
        "                    The data must be passed to the algorithm as 2-d array, \n",
        "                    where columns are the features and the rows are the samples\n",
        "        '''\n",
        "        # Spliting the data in n_componets sub-sets\n",
        "        new_X = np.array_split(X, self.n_componets)\n",
        "        # Initial computation of the mean-vector and covarience matrix\n",
        "        self.mean_vector = [np.mean(x, axis=0) for x in new_X]\n",
        "        self.covariance_matrixes = [np.cov(x.T) for x in new_X]\n",
        "        # Deleting the new_X matrix because we will not need it anymore\n",
        "        del new_X\n",
        "        for iteration in range(self.max_iter):\n",
        "            ''' --------------------------   E - STEP   -------------------------- '''\n",
        "            # Initiating the r matrix, evrey row contains the probabilities\n",
        "            # for every cluster for this row\n",
        "            self.r = np.zeros((len(X), self.n_componets))\n",
        "            # Calculating the r matrix\n",
        "            for n in range(len(X)):\n",
        "                for k in range(self.n_componets):\n",
        "                    self.r[n][k] = self.pi[k] * self.multivariate_normal(X[n], self.mean_vector[k], self.covariance_matrixes[k])\n",
        "                    self.r[n][k] /= sum([self.pi[j]*self.multivariate_normal(X[n], self.mean_vector[j], self.covariance_matrixes[j]) for j in range(self.n_componets)])\n",
        "                    # print(self.multivariate_normal(X[n], self.mean_vector[k], self.covariance_matrixes[k]))\n",
        "            # Calculating the N\n",
        "            N = np.sum(self.r, axis=0)\n",
        "            ''' --------------------------   M - STEP   -------------------------- '''\n",
        "            # Initializing the mean vector as a zero vector\n",
        "            self.mean_vector = np.zeros((self.n_componets, len(X[0])))\n",
        "            # Updating the mean vector\n",
        "            for k in range(self.n_componets):\n",
        "                for n in range(len(X)):\n",
        "                    self.mean_vector[k] += self.r[n][k] * X[n]\n",
        "            self.mean_vector = [1/N[k]*self.mean_vector[k] for k in range(self.n_componets)]\n",
        "            # Initiating the list of the covariance matrixes\n",
        "            self.covariance_matrixes = [np.zeros((len(X[0]), len(X[0]))) for k in range(self.n_componets)]\n",
        "            # Updating the covariance matrices\n",
        "            for k in range(self.n_componets):\n",
        "                self.covariance_matrixes[k] = np.cov(X.T, aweights=(self.r[:, k]), ddof=0)\n",
        "            self.covariance_matrixes = [1/N[k]*self.covariance_matrixes[k] for k in range(self.n_componets)]\n",
        "            # Updating the pi list\n",
        "            self.pi = [N[k]/len(X) for k in range(self.n_componets)]\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "            The predicting function\n",
        "                :param X: 2-d array numpy array\n",
        "                    The data on which we must predict the clusters\n",
        "        '''\n",
        "        probas = []\n",
        "        for n in range(len(X)):\n",
        "            probas.append([self.multivariate_normal(X[n], self.mean_vector[k], self.covariance_matrixes[k])\n",
        "                           for k in range(self.n_componets)])\n",
        "        \n",
        "        cluster = []\n",
        "        for proba in probas:\n",
        "            cluster.append(self.comp_names[proba.index(max(proba))])\n",
        "        return cluster"
      ],
      "metadata": {
        "id": "IjtFRFDdTXUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm = GMMl(2)\n",
        "gm.fit(train_data)"
      ],
      "metadata": {
        "id": "iSQ5XgSFsBMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm.mean_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8U0ooXKsR0S",
        "outputId": "6c226ab9-761c-41b7-d6db-6a716a97c95a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([5.14153846, 3.18461538, 2.01384615, 0.45538462]),\n",
              " array([6.38      , 2.96      , 5.09176471, 1.76823529])]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gm.mean_vector"
      ],
      "metadata": {
        "id": "ehMSHtGWs9p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jsAsqZokih5x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}